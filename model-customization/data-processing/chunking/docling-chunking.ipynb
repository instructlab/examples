{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290ef089-c1b5-4a95-8a0e-1c650bac5ea2",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "The goal of chunking converted documents is to break them down into smaller and logical pieces.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d748e-4895-4157-ac5a-1a323871c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675f6d0-1e53-449a-ae26-c49b060fb3a9",
   "metadata": {},
   "source": [
    "### Set directory for files to convert and output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3270228-a20b-40ee-93b5-4cb5623c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sample_data_dir = Path(\"data/sample-docling-json\")\n",
    "docling_json_files = list((sample_data_dir.glob(\"*.json\")))\n",
    "\n",
    "output_dir = Path(\"data/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f58475-31be-452a-955d-48072b7656e9",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and `merge_peers` to merge undersized chunks that are next to eachother. Uncomment the commented out code to configure these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5c0d1-6a8f-472a-b084-1ceb891d6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#MAX_TOKENS = 1024\n",
    "#\n",
    "# tokenizer = HuggingFaceTokenizer(\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "#     max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    "#     merge_peers=True # \n",
    "# )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    #tokenizer=tokenizer,\n",
    "    #merge_peers=True,  # whether to merge undersized chunks - defaults to True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8efec4-80de-4f1a-827a-1264d3a462a1",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document.\n",
    "\n",
    "The resulting chunks are stored in a file called chunks.jsonl in the `chunks` directory in your contribution. This file is used as an input in a later step when creating the seed dataset for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf2e97-ca75-4d49-8a4c-6f7931f7f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "all_chunks = []\n",
    "    \n",
    "for file in docling_json_files:\n",
    "    # reconvert the docling JSON for chunking\n",
    "    doc = DocumentConverter().convert(source=file)\n",
    "\n",
    "    document_chunks = []\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "    chunk_objs = list(chunk_iter)\n",
    "\n",
    "    print(f\"Extracted {len(chunk_objs)} chunks from {doc.document.name}\")\n",
    "    \n",
    "    for chunk in chunk_objs:\n",
    "        c = dict(chunk=chunker.contextualize(chunk=chunk), file=doc.document.name,metadata=chunk.meta.export_json_dict())\n",
    "        document_chunks.append(c)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "    document_chunk_dir = output_dir / f\"{doc.document.name}\"\n",
    "    document_chunk_dir.mkdir(parents=True, exist_ok=True)\n",
    "    chunks_file_path = document_chunk_dir / \"chunks.jsonl\"\n",
    "    with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for chunk in document_chunks:\n",
    "            json.dump(chunk, file)\n",
    "            file.write(\"\\n\")\n",
    "        print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d0e62-d4fa-4a8b-8281-18a7eb2671a2",
   "metadata": {},
   "source": [
    "### View the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8aa1-43a6-4218-bb49-0d35fb68ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_gen = iter(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a224af-84d5-412f-a489-05ac07eacb1b",
   "metadata": {},
   "source": [
    "The document is now broken into small sections with metadata about the chunk based on the document's format. To view the chunks one by one, rerun the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d39f2-3c94-4df2-9ab7-81d512e49c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(chunk_gen)['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556529b-8c41-41f7-a80f-ebd756e54180",
   "metadata": {},
   "source": [
    "To view several randomly selected chunks, run the following cell as many times as you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e22f8-8a67-414a-a554-e57fe4396f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_VIEW = 5\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "sample = random.sample(all_chunks, min(len(all_chunks), NUM_CHUNKS_TO_VIEW))\n",
    "\n",
    "i = 1\n",
    "for chunk in sample:\n",
    "    print(f\"== Randomly selected chunk {i}: ==========\\n\\n{chunk['chunk']}\\n\\n\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
