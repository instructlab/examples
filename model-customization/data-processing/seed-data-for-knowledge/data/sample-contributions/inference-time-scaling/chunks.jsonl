{"chunk": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\nIsha Puri 1 Shivchander Sudalairaj 2 Guangxuan Xu 2 Kai Xu 2 Akash Srivastava 2 1 MIT CSAIL 2 Red Hat AI Innovation\nc", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/2", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 115.114, "t": 634.227, "r": 477.615, "b": 612.034, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 115]}]}, {"self_ref": "#/texts/3", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 372.747, "t": 608.794, "r": 377.059, "b": 600.088, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}]}], "headings": ["A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Abstract\nLarge language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT4o accuracy in only 4 rollouts, while Qwen2.5Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/5", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 75.017, "t": 568.205, "r": 271.256, "b": 189.043, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1540]}]}], "headings": ["Abstract"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Abstract\nonly presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at probabilistic-inference-scaling.github.io/ .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/5", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 75.017, "t": 568.205, "r": 271.256, "b": 189.043, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1540]}]}], "headings": ["Abstract"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nLarge language models (LLMs) have demonstrated remarkable improvements in performance through scaling up\n1 MIT CSAIL 2 Red Hat AI Innovation. Correspondence to: Isha Puri < ishapuri@mit.edu > , Shivchander Sudalairaj < ssudalai@redhat.com > .\nCopyright 2025 by the author(s).\nFigure 1. State-space model for inference-time scaling. c is a prompt, x 1 , . . . , x T are sequence of partial LLM outputs and o 1 , . . . , o T are the 'observed' acceptance. We cast inferencetime scaling as to estimate the latent states conditioned on o t = 1 for t = 1 , 2 , . . . , T , i.e. all being accepted.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/7", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 55.44, "t": 154.24, "r": 291.093, "b": 133.73299999999995, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 104]}]}, {"self_ref": "#/texts/8", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "footnote", "prov": [{"page_no": 1, "bbox": {"l": 55.44, "t": 124.49000000000001, "r": 290.929, "b": 95.005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 137]}]}, {"self_ref": "#/texts/9", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 55.44, "t": 82.77599999999995, "r": 173.151, "b": 75.07899999999995, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}]}, {"self_ref": "#/texts/10", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 306.938, "t": 471.521, "r": 542.932, "b": 419.701, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 316]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nmodel sizes and/or data. While frontier models have relied heavily on larger datasets and an ever-increasing number of learnable parameters (Kaplan et al., 2020; Snell et al., 2024), smaller LLMs have successfully leveraged domain-specific data to match the performance of larger, general-purpose models (Sudalairaj et al., 2024; Pareja et al., 2024). However, recent reports indicate plateaus in performance gains through such scaling methods. Consequently, inferencetime (aka compute-time / test-time) scaling has emerged as a promising alternative to improve model performance (Beeching et al., 2024). Proprietary models like OpenAI's o1 (OpenAI et al., 2024) and o3 have demonstrated the benefits of allocating more computation resources at inference time, particularly for complex reasoning and math tasks. These inference-time scaling techniques not only enhance model capability but also allow smaller models to achieve performance levels comparable to their larger counterparts, making advanced AI more accessible for low-resource devices.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/82", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 307.111, "t": 400.237, "r": 543.185, "b": 176.49199999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1047]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nRecent work (Lightman et al., 2023a) has framed inferencetime scaling as a search problem guided by a process reward model (PRM). This perspective has led to the successful application of classic algorithms such as best-of-n (BoN; Brown et al., 2024), beam search (Zhou et al., 2024; Snell et al., 2024), and Monte Carlo tree search (MCTS; Guan et al., 2025), which refine model outputs by systematically exploring a broader search space. This process is sometimes\n(a) Llama-3.2-1B-Instruct\n(b) Llama-3.1-8B-Instruct\n(c) Qwen2.5-Math-1.5B-Instruct\n(d) Qwen2.5-Math-7B-Instruct", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/83", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 307.44, "t": 167.111, "r": 543.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 464]}]}, {"self_ref": "#/texts/109", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 125.731, "t": 612.335, "r": 218.856, "b": 604.638, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 25]}]}, {"self_ref": "#/texts/136", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 377.479, "t": 612.335, "r": 471.106, "b": 604.638, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 25]}]}, {"self_ref": "#/texts/161", "parent": {"$ref": "#/pictures/4"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 113.653, "t": 489.933, "r": 230.934, "b": 482.236, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 30]}]}, {"self_ref": "#/texts/188", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 368.764, "t": 489.933, "r": 479.822, "b": 482.236, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 28]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nFigure 2. Performance of PF compared to other inference-time scaling methods across different model families. Figure 2a and Figure 2b demonstrate results for the Llama-3 family, where PF outperforms WBoN and DVTS in both cases and approaches the performance of much larger models like Llama-3.1-70B and even GPT-4o . Figure 2c and Figure 2d show results for the Qwen family, where PF achieves superior scaling against baslines, enabling the smaller model Qwen2.5-Math-1.5B-Instruct to surpass GPT-4o in performance within a limited compute budget. Larger Qwen2.5-Math-7B-Instruct model efficiently scale to match o1-preview performance on MATH500.\nreferred to as 'thinking/reasoning'.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/189", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 54.938, "t": 469.871, "r": 541.444, "b": 418.338, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 647]}]}, {"self_ref": "#/texts/190", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 55.44, "t": 409.799, "r": 196.54, "b": 401.247, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 36]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nHowever, we argue that a search-based formulation becomes problematic when the reward model is imperfect-an inherent issue since these models are only approximations of an unknown true classification or preference function. Empirically, this often leads to reward hacking, where the final output is optimized to score well according to the reward model but fails to be useful and/or correct (Snell et al., 2024).\nThe idea of using more computation to refine results is a fundamental feature of many classic probabilistic inference methods. For instance, Markov chain Monte Carlo (MCMC) methods improve inference asymptotically with more iterations, while particle-based Monte Carlo methods enhance accuracy as the number of particles increases.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/191", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 55.44, "t": 391.866, "r": 291.098, "b": 311.583, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 412]}]}, {"self_ref": "#/texts/192", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 307.131, "t": 409.799, "r": 543.095, "b": 341.471, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 331]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nIn this paper, we propose a shift in perspective by framing inference-time scaling as a probabilistic inference task. Unlike search-based methods that seek the mode of the reward model's distribution, we leverage sampling-based techniques to explore the typical set, which is more likely to overlap with the ground truth. This approach reduces reliance on potentially flawed reward models, as probabilistic inference naturally balances exploitation and exploration by trusting the reward model only up-to a certain probability (Andrieu et al., 2010). More specifically, unlike existing search-based methods in inference-time scaling, our probabilistic approach to scaling strikes a unique balance between exploration and exploitation. If the search process discovers a partial solution with a high process reward score, the next step will resample that solution more heavily but will typically not have it completely dominate the next step of particles, allowing for more diverse options to still continue their exploration.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/193", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 55.44, "t": 302.202, "r": 291.185, "b": 90.41300000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1024]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\nBuilding on this principle, we introduce a novel approach to inference-time scaling by adapting particle-based Monte Carlo algorithms from probabilistic inference. Our method explicitly accounts for imperfections in reward models by maintaining a diverse set of candidates within the solution space. By iteratively updating their weights based on observed evidence (approximate reward), our approach ensures robust scaling even when the reward model is imperfect.\nOur key contributions are as follows.\n1. We formulate inference-time scaling as probabilistic inference over a state space model (SSM) jointly defined by a language model (transition kernel) and a process reward model (emission model), which enables direct application of probabilistic inference methods.\n2. Wepropose inference-time scaling algorithms based on the particle filtering (PF) algorithm, which is robust to imperfection in reward modeling. We study its scaling performance and the effective temperature in LLM generation and how to optimally allocate computation\nbudget over its multi-iteration and parallel extensions.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/194", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 307.44, "t": 332.09, "r": 543.093, "b": 239.85199999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 463]}]}, {"self_ref": "#/texts/195", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 307.44, "t": 230.471, "r": 454.498, "b": 221.91899999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 37]}]}, {"self_ref": "#/texts/196", "parent": {"$ref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 2, "bbox": {"l": 314.912, "t": 202.71000000000004, "r": 543.097, "b": 146.337, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 266]}]}, {"self_ref": "#/texts/197", "parent": {"$ref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 2, "bbox": {"l": 314.912, "t": 131.24599999999998, "r": 541.443, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 269]}]}, {"self_ref": "#/texts/199", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 75.366, "t": 721.463, "r": 290.279, "b": 712.911, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 56]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "1. Introduction\n3. We study ways to use PRMs and propose a more robust and performant way to obtain rewards for partial answers which we refer to as model-based aggregation.\n4. We demonstrate that the proposed methods have 416x faster scaling speed than previous methods based on a search formulation on the MATH500 and AIME 2024 datasets, with small language models in the Llama and Qwen families. We show that PF can scale Qwen2.5-Math-1.5B-Instruct to surpasses GPT-4o accuracy with only a budget of 4 and scale Qwen2.5Math-7B-Instruct to o1 accuracy with a budget of 32.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/200", "parent": {"$ref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 3, "bbox": {"l": 62.912, "t": 701.382, "r": 291.185, "b": 668.919, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 157]}]}, {"self_ref": "#/texts/201", "parent": {"$ref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 3, "bbox": {"l": 62.912, "t": 657.391, "r": 291.092, "b": 565.152, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 400]}]}], "headings": ["1. Introduction"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "2. Related Work\nProcess reward models (PRMs) aim to provide more granular feedback by evaluating intermediate steps rather than only final outputs. They are trained via process supervision, a training approach where models receive feedback on each intermediate step of their reasoning process rather than only on the final outcome. Lightman et al. (2023a) propose a step-by-step verification approach to PRMs, improving the reliability of reinforcement learning. DeepSeek PRM (Wang et al., 2024) uses Mistral to annotate training data for PRMs . Zhang et al. (2025b) introduces Qwen-PRM, which combines both Monte Carlo estimation and model/human annotation approach to prepare training data for a PRM. PRIME (Cui et al., 2025) proposes to train an outcome reward model (ORM) using an implicit reward objective. The paper shows that implicit reward objective directly learns a Q-function that provides rewards for each token, which can be leveraged to create process-level reward signal. This process eliminates the need for any process labels, and reaches competitive performance on PRM benchmarks.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/203", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 55.082, "t": 526.155, "r": 291.185, "b": 302.021, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1083]}]}], "headings": ["2. Related Work"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "2. Related Work\nInference-time scaling has been a key training-free strategy for enhancing LLM performance. Brown et al. (2024) explores a best-of-N (BoN) decoding strategy, demonstrating improvements in output quality through selective refinement. (Snell et al., 2024) provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective. While not implementing full Monte Carlo tree search (MCTS), Zhou et al. (2024) explores a tree-search-like approach within language models. Additionally, Guan et al. (2025) introduces rSTAR, a method that combines MCTS for data generation and training to improve mathematical reasoning. Beeching et al. (2024) discusses beam search and dynamic variable-time search (DVTS) as inference-time scaling techniques to improve open-source LLMs. DVTS works by running multiple independent subtrees in parallel so to avoid all leaves stuck in local minima.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/204", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 55.112, "t": 293.029, "r": 291.186, "b": 92.80600000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 931]}]}], "headings": ["2. Related Work"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "2. Related Work\nParticle-based Monte Carlo methods are powerful tools for probabilistic inference. Sequential Monte Carlo (Moral, 1997) or particle filtering (Swendsen & Wang, 1986) has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling (Andrieu et al., 2010) extends these approaches by integrating MCMC techniques for improved inference. (Lew et al., 2023) and (Loula et al., 2025) introduce a probabilistic programming language that applies SMC methods to steer/constrain LLM generation. (Zhao et al., 2024) and (Feng et al., 2024) introduce Twisted SMC methods for inference in language models.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/205", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 55.44, "t": 83.81299999999999, "r": 289.444, "b": 74.85699999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 53]}, {"page_no": 3, "bbox": {"l": 306.693, "t": 721.463, "r": 543.185, "b": 593.359, "coord_origin": "BOTTOMLEFT"}, "charspan": [54, 655]}]}], "headings": ["2. Related Work"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "3. Background\nState space models are a class of probabilistic models used to describe sequential systems that evolve stepwise, typically over time (S\u00a8 arkk\u00a8 a, 2013). They consist of a sequence of hidden states { x t } T t =1 and corresponding observations { o t } T t =1 , where x t \u2208 X represents the latent state at step t , and o t \u2208 Y is the observation. The evolution of states is governed by a transition model p ( x t | x <t -1 ) , and the observations are governed by the emission model p ( o t | x t ) . The joint distribution of states and observations is given by: p ( x 1: T , o 1: T ) = p ( x 1 ) \u220f T t =2 p ( x t | x <t -1 ) \u220f T t =1 p ( o t | x t ) , where p ( x 1 ) is the prior distribution over the initial state.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/207", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 307.082, "t": 554.361, "r": 543.184, "b": 425.11, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 718]}]}], "headings": ["3. Background"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "3. Background\nProbabilistic inference in SSMs involves estimating the posterior distribution of the hidden states given the observations, p ( x 1: T | o 1: T ) (S\u00a8 arkk\u00a8 a, 2013). This task is generally intractable due to the high dimensionality of the state space and the dependencies in the model. Common approaches approximate the posterior through sampling-based methods or variational approaches (MacKay, 2003).\nParticle filtering (PF) is a sequential Monte Carlo method to approximate the posterior distribution in SSMs (Swendsen & Wang, 1986; Moral, 1997). PF represents the posterior using a set of N weighted particles { x ( i ) t , w ( i ) t } N i =1 , where x ( i ) t denotes the i th particle at time t , and w ( i ) t is its associated weight. The algorithm iteratively propagates particles using the transition model and updates weights based on the emission model: w ( i ) t \u221d w ( i ) t -1 p ( o t | x ( i ) t ) .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/208", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 307.191, "t": 416.877, "r": 543.093, "b": 336.206, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 402]}]}, {"self_ref": "#/texts/209", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 307.082, "t": 327.213, "r": 543.095, "b": 227.41899999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 511]}]}], "headings": ["3. Background"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4. Method\nWe begin by formulating inference-time scaling for LLMs as probabilistic inference over a state-space model (SSM), where the transition kernel is defined by the LLM and the emission probabilities are given by the PRM (Section 4.1). Next, in Section 4.2, we introduce how particle filtering (PF) can be applied to this inference task. We then extend our approach to incorporate multiple iterations and parallel chains, providing more ways to allocate computation budgets.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/211", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 306.972, "t": 189.95399999999995, "r": 543.188, "b": 97.71600000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 470]}]}], "headings": ["4. Method"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs\nFor a LLM M (or p M ), our approach to inference-time scaling attempts to estimate the latent states of the following joint distribution over tokens (or chunks, e.g. steps in math problems) x 1: T and observations o 1: T representing the acceptance of the tokens, given prompt c\n<!-- formula-not-decoded -->\n\u00b7 The transition kernel p M ( x t | c, x <t -1 ) is defined by M ;\n\u00b7 The emission model or likelihood p ( o t | c, x t ) = B ( o t ; r ( c, x t )) is a Bernoulli whose parameter is defined by a reward function r of each x t for prompt c .\nFigure 1 shows the plate diagram of this SSM we define.\nIn inference-time scaling, we would like to find the sequence of latent states such that all steps are accepted ( o t = 1 for all t ), i.e. estimating p M ( x 1: T | c, o 1: T = 1 ) . This interpretation makes PF directly applicable.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/214", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 55.44, "t": 691.173, "r": 291.093, "b": 634.482, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 278]}]}, {"self_ref": "#/texts/215", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "formula", "prov": [{"page_no": 4, "bbox": {"l": 68.718, "t": 621.229, "r": 290.111, "b": 575.673, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 103]}]}, {"self_ref": "#/texts/216", "parent": {"$ref": "#/groups/2"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 4, "bbox": {"l": 55.44, "t": 558.263, "r": 290.267, "b": 548.633, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 66]}]}, {"self_ref": "#/texts/217", "parent": {"$ref": "#/groups/2"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 4, "bbox": {"l": 55.44, "t": 546.308, "r": 289.438, "b": 512.768, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 171]}]}, {"self_ref": "#/texts/218", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 55.44, "t": 503.15, "r": 281.272, "b": 494.598, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 55]}]}, {"self_ref": "#/texts/219", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 55.44, "t": 485.217, "r": 290.441, "b": 440.8, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 233]}]}], "headings": ["4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs\nFurther, as the optimal or the ground-truth reward function r is often unknown in practice, we approximate r via a model that is suitable for the task. Following previous works, we use pre-trained PRMs \u02c6 r for such approximation when solving reasoning tasks in the domain of mathematics (for example), which gives us an approximate likelihood \u02c6 p ( o t | c, x t ) = B ( o t ; \u02c6 r ( c, x t )) . Thus, our task is to estimate the latent states of the following joint given o t = 1 for all t\n<!-- formula-not-decoded -->", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/220", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 55.082, "t": 431.737, "r": 290.687, "b": 338.422, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 488]}]}, {"self_ref": "#/texts/221", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "formula", "prov": [{"page_no": 4, "bbox": {"l": 87.304, "t": 325.928, "r": 290.107, "b": 280.372, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 101]}]}], "headings": ["4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs\nSampling v.s. search An alternative to our samplingbased approach would be to find a point estimation of the distribution via optimization, which essentially reduces to variants of existing search-based inference-time scaling methods like MCTS, beam search, etc. However, we argue that such search-based methods are not robust in the case of PRM-based noisy approximations to the reward function. On the other hand, sampling using (2) can produce a closer estimation of (1) than optimization. This can be understood by comparing the typical set and the mode of a distribution: the mode of (2) is more sensitive to approximation errors in \u02c6 r than the typical set. This aligns with the classic insight that while sampling-based methods remain invariant to reparameterization in the likelihood, maximum-a-posteriori (MAP) inference-which underlies search-based methods-does not (Murphy, 2012).\nIn essence, sampling-based approaches are more robust to approximation errors in the likelihood, making them a better fit for this task-an advantage we will demonstrate empirically in the following sections.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/222", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 55.082, "t": 263.1410000000001, "r": 291.187, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 891]}]}, {"self_ref": "#/texts/223", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 307.44, "t": 721.463, "r": 541.437, "b": 677.045, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 207]}]}], "headings": ["4.1. Inference-time scaling LLMs with PRMs as probabilistic inference over SSMs"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.2. Particle filtering for inference-time scaling\nWe now consider inference-time scaling with an LLM p M and a PRM \u02c6 r via sampling from the posterior of (2) by conditioning on accepting all steps. The direct application of the classic particle filtering algorithm to this inference-time scaling setup requires defining the following components.\n\u00b7 State initialization and transition p M ( x t | c, x <t -1 ) is done by prompting the LLM with the prompt c to generate responses for a single step. These steps are determined automatically through stop-token delimiters. The LLM temperature is a hyperparameter to tune optimally for different tasks (see ablation in Section 5.4);\n\u00b7 Weight update w ( i ) t \u221d w ( i ) t -1 \u02c6 r ( x ( i ) t ) uses the PRM to compute the reward per step, as detailed next.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/225", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 306.972, "t": 642.13, "r": 543.098, "b": 585.439, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 295]}]}, {"self_ref": "#/texts/226", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 4, "bbox": {"l": 307.44, "t": 581.357, "r": 541.614, "b": 512.712, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 331]}]}, {"self_ref": "#/texts/227", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 4, "bbox": {"l": 307.44, "t": 511.546, "r": 543.092, "b": 487.586, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 121]}]}], "headings": ["4.2. Particle filtering for inference-time scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "PRMs for likelihood estimation and weight update\nHow to aggregate the step-level rewards remains a choice when one uses PRMs. There are three common ways to assign rewards to a partial answer using PRMs: prod , which takes the product of rewards across all steps; min , which selects the minimum reward over all steps; and last , which uses the reward from the final step. Zhang et al. (2025b) studies the optimal way for reward aggregation and points out that the 'best choice' depends on if the PRM training data is prepared using MC rollout and/or human/model annotation. While prod aligns directly with the weight update rule described earlier, min and last do not allow for online weight updates. Therefore, for these methods, we compute the weight based on the entire partial trajectory instead.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/229", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 307.082, "t": 448.058, "r": 543.096, "b": 296.044, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 752]}]}], "headings": ["PRMs for likelihood estimation and weight update"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "PRMs for likelihood estimation and weight update\nBeyond these three approaches, we also explored a modelbased reward aggregation method that performed surprisingly well. This method feeds the PRM with partial answers but only considers the final reward token, effectively prompting the model to provide an aggregated reward for the partial answer. Interestingly, we tested the Qwen PRM both for its original purpose as a true process reward model and repurposed as an outcome reward model. When used as a true PRM, it receives the question and a list of steps generated by the policy model, calculates scores for each step and selects the last score-a practice introduced and evaluated in Beeching et al. (2024). As an ORM, the PRM takes in a question and a concatenated string of generated steps, producing a score that we convert into a weight for the resampling process. Appendix A.2 provides an illustration of how the two input formats are structured. We compare various reward models and evaluate all four aggregation strategies through an ablation study in Section 5.4.\nWith the above defined, particle filtering iterates over the two steps below with a set of N particles at each iteration t", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/230", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 307.44, "t": 286.663, "r": 543.098, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1027]}]}, {"self_ref": "#/texts/232", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 54.972, "t": 721.463, "r": 289.439, "b": 700.955, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 122]}]}], "headings": ["PRMs for likelihood estimation and weight update"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "PRMs for likelihood estimation and weight update\n\u00b7 Propagation: We start by propagating the set of particles S t -1 via initialization ( t = 1 ) or transition ( t > 1 ) and calculate their weights. This produces a set of weighted particles S \u2032 t = { x ( i ) t , w ( i ) t } , which represents partial generation upto step t and their importance;\n\u00b7 Resampling: We sample with replacement over the particles to produce a new set of particles with the same number. Specifically, let the resampling distribution (over index j ) be\n<!-- formula-not-decoded -->\nWe sample { j ( i ) t \u223c P t ( j = i ) } i M =1 and obtain a new set of particles S t = { x j ( i ) t t , w j ( i ) t t } . This step is essentially a probabilistic search with higher chances to explore high reward partial generations: These weights do not blindly guide the selection of high-reward particles at every stage of the search-they retain a degree of stochasticity that encourages exploration of under-explored regions of the sample space-explorations that may discover higher value answers later on.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/233", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 696.874, "r": 291.099, "b": 638.58, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 296]}]}, {"self_ref": "#/texts/234", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 635.495, "r": 291.099, "b": 590.759, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 180]}]}, {"self_ref": "#/texts/235", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "formula", "prov": [{"page_no": 5, "bbox": {"l": 90.203, "t": 582.485, "r": 290.107, "b": 562.415, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 70]}]}, {"self_ref": "#/texts/236", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 64.935, "t": 553.116, "r": 289.786, "b": 441.004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 511]}]}], "headings": ["PRMs for likelihood estimation and weight update"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "PRMs for likelihood estimation and weight update\nNote that the resampling step in particle filtering maintains a natural balance between exploiting promising hypotheses and exploring less-certain regions that may yield novel solutions. By maintaining a diverse population of particles and dynamically adjusting their weights at each step, our method allows a level of flexibility that is absent in traditional strategies, such as greedy search or beam search. In general, the ability to guide exploration using PRM-based scores allows the framework to harness the strengths of reward models without being limited by their flaws.\nImportantly, this approach ensures that inference scaling remains fruitful within smaller compute budgets, as the resampling and unrolling operations are computationally efficient and can be parallelized across particles. With proper prefix caching, the total computation on generation is as much as that for generating N complete answers directly.\nFigure 3 provides an illustration of the method with 4 particles with comparison to beam search, and the overall algorithm is detailed in Algorithm 1.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/237", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 430.627, "r": 291.186, "b": 314.479, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 579]}]}, {"self_ref": "#/texts/238", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 305.098, "r": 291.09, "b": 236.76999999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 348]}]}, {"self_ref": "#/texts/239", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 227.389, "r": 291.098, "b": 194.92700000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 150]}]}], "headings": ["PRMs for likelihood estimation and weight update"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS\nThe PF approach to inference-time scaling can be used to define a MCMC kernel that enables two new types of scaling: multiple iterations of complete answers inspired by PG and parallel simulations inspired by parallel tempering.\nParticle Gibbs is a type of MCMC algorithm that uses PF as a transition kernel (Andrieu et al., 2010). Specifically, at each iteration, PG samples a new set of particles\n(a) Particle filtering uses the rewards to produce a softmax distribution and does stochastic expansion of N based sampling.\n(b) Beam search treats the rewards as exact and performs deterministic expansion based on beam size N and beam width M .\nFigure 3. A side-by-side comparison between particle filtering and its closet search-based counterpart, beam search. Compared with beam search in Figure 3b where the selection and expansion is deterministic (implicitly assumes the rewards are correct), particle filtering in Figure 3a trust the rewards with uncertainty and propagate the expansion via sampling. A more detailed, stepby-step version of particle filtering can be found in Figure 9 of Appendix A.1.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/241", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 55.131, "t": 161.13400000000001, "r": 290.647, "b": 116.71600000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 228]}]}, {"self_ref": "#/texts/242", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 55.44, "t": 107.72400000000005, "r": 291.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 169]}]}, {"self_ref": "#/texts/243", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 5, "bbox": {"l": 307.505, "t": 616.143, "r": 542.567, "b": 598.484, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 124]}]}, {"self_ref": "#/texts/261", "parent": {"$ref": "#/pictures/6"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 5, "bbox": {"l": 307.505, "t": 487.562, "r": 542.569, "b": 469.903, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 120]}]}, {"self_ref": "#/texts/276", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 306.938, "t": 457.537, "r": 542.933, "b": 373.128, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 462]}]}], "headings": ["4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS\nusing PF with a reference particle from the previous iteration. This integration combines the efficiency of PF with the theoretical guarantees of MCMC, making PG suitable for high-dimensional or challenging posterior distributions. The adaption of PG to inference-time scaling is essentially a multi-iteration extension of the PF algorithm presented, which works as follows: For each iteration, we run a modified PF step with an additional sampling step to sample 1 reference particle according to (3). For any PF step that is not the initial step, the PF is executed with a reference particle: This reference particle is never replaced during the resampling step, but its partial trajectory can still be forked during resampling. We detail the PG version of inferencetime scaling in Algorithm 2 of Appendix A.1. Note that typically, a reasonably large number of particles is needed to show the benefits of multiple iterations, which we also confirm in our results in Section 5.4.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/277", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 306.693, "t": 364.589, "r": 543.181, "b": 164.75400000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 980]}]}], "headings": ["4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS\nParallel tempering In parallel tempering (aka replica exchange MCMC sampling), multiple MCMC chains run in parallel at different temperatures and swap the states to allow better exploration. The key idea is that the chain running in high temperature can explore better, e.g. traversing between different modes of the target, and the swap makes it possi-\nAlgorithm 1 Particle Filtering for Inference-Time Scaling\nInput : the number of particles N , a reward model \u02c6 r , a LLM p and the prompt c", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/278", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 307.44, "t": 143.58899999999994, "r": 543.097, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 353]}]}, {"self_ref": "#/texts/280", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 55.082, "t": 722.16, "r": 287.001, "b": 713.204, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 57]}]}, {"self_ref": "#/texts/281", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 65.403, "t": 706.708, "r": 289.44, "b": 685.813, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 81]}]}], "headings": ["4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS\nM Initialize N particles { x ( i ) 1 \u223c p M ( \u00b7 | c ) } N i =1 t \u2190 1 while not all particles stop do Update rewards w = [\u02c6 r ( x (1) 1: t ) , . . . , \u02c6 r ( x ( N ) 1: t )] Compute softmax distribution \u03b8 = softmax( w ) Sample indices { j ( i ) t } N i =1 \u223c P t ( j = i ) = \u03b8 i Update the set of particles as { x ( j ( i ) t ) 1: t } N i =1 Transition { x ( i ) t +1 \u223c p M ( \u00b7 | c, x ( i ) 1: t ) } N i =1 t \u2190 t +1 end while\nReturn : the set of particles in the end ble to let the low temperature chain exploit the new region found by the other chain. We detail the complete parallel tempering version of inference-time scaling in Algorithm 3 of Appendix A.1 while we only explore a special case of it (multiple chains with single iteration) in our experiments.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/282", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 65.403, "t": 691.148, "r": 270.989, "b": 554.532, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 421]}]}, {"self_ref": "#/texts/283", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 65.403, "t": 551.533, "r": 216.097, "b": 542.577, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 40]}, {"page_no": 6, "bbox": {"l": 55.112, "t": 514.717, "r": 289.442, "b": 458.344, "coord_origin": "BOTTOMLEFT"}, "charspan": [41, 336]}]}], "headings": ["4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5. Evaluation\nWe thoroughly evaluate our proposed methods in this section. We detail our experimental setup in Section 5.1 and start with highlighted results on comparison with other closed-source models and competitive inference-time scaling methods with open-source models (Section 5.2). We then study how the main algorithm, particle filtering, scales with more computation and compare it with its competitors (Section 5.3). We further perform an extensive ablation study on key algorithmic choices like reward models, reward aggregation and LLM temperatures (Section 5.4). We finally study different possible allocations of the computation budget through iterative and parallel extensions (Section 5.5).", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/285", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.972, "t": 418.958, "r": 291.098, "b": 278.899, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 693]}]}], "headings": ["5. Evaluation"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.1. Setup\nModels We consider two types of open-source small language models (SLMs) as our policy models for generating solutions. The first is general-purpose models, of which we used Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct (Grattafiori et al., 2024). The second is math-specialized models, where we used Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-Instruct (Yang et al., 2024). These small models are well-suited for inference-time scaling, enabling efficient exploration of multiple trajectories.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/287", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 55.082, "t": 244.05399999999997, "r": 291.093, "b": 139.47299999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 499]}]}], "headings": ["5.1. Setup"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.1. Setup\nProcess Reward Models To guide our policy models, we utilized Qwen2.5-Math-PRM-7B (Zhang et al., 2025a), a 7B process reward model. We selected this model because it demonstrated superior performance compared to other PRMs we tested, including Math-Shepherd-mistral-7bprm (Wang et al., 2024), Llama3.1-8B-PRM-Deepseek-Data (Xiong et al., 2024), and EurusPRM-Stage2 (Yuan et al., 2024). This result as an ablation study is provided in Section 5.4, where we also study the different ways to aggregate step-level rewards from PRMs discussed in Section 4.2.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/288", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 55.082, "t": 119.67899999999997, "r": 291.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 211]}, {"page_no": 6, "bbox": {"l": 307.112, "t": 721.463, "r": 543.098, "b": 653.135, "coord_origin": "BOTTOMLEFT"}, "charspan": [212, 553]}]}], "headings": ["5.1. Setup"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Baselines\n\u00b7 Pass@1: single greedy generation from the model, serving as the 'bottom-line' performance.\n\u00b7 BoN/WBoN (Brown et al., 2024): (weighted) best-of-N is the most straightforward inference-time scaling method using reward models.\n\u00b7 DVTS (Beeching et al., 2024): a parallel extension of beam search that improves the exploration hence overall scaling performance. 1\nDatasets To evaluate our methods and baselines, we consider widely-used datasets spanning multiple domains and difficulty levels and challenging benchmarks, ensuring a robust assessment of the methods' performance across basic and advanced problem-solving and reasoning tasks.\n\u00b7 MATH500 (Lightman et al., 2023b): A dataset containing 500 high-difficulty competition-level problems from various mathematical domains.\n\u00b7 AIME2024 (AI-MO, 2023): A collection of 30 problems from the American Invitational Mathematics Examination (AIME I and II) 2024.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/290", "parent": {"$ref": "#/groups/5"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 630.324, "r": 543.09, "b": 609.498, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 92]}]}, {"self_ref": "#/texts/291", "parent": {"$ref": "#/groups/5"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 606.413, "r": 541.443, "b": 573.633, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 132]}]}, {"self_ref": "#/texts/292", "parent": {"$ref": "#/groups/5"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 570.548, "r": 541.441, "b": 537.767, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 134]}]}, {"self_ref": "#/texts/293", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 522.0, "r": 543.092, "b": 465.239, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 276]}]}, {"self_ref": "#/texts/294", "parent": {"$ref": "#/groups/6"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 461.228, "r": 543.091, "b": 428.378, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 138]}]}, {"self_ref": "#/texts/295", "parent": {"$ref": "#/groups/6"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 425.362, "r": 541.443, "b": 392.512, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 130]}]}], "headings": ["Baselines"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Baselines\nParsing and scoring To evaluate model-generated responses, we enforce a structured answer format using a system prompt (see Appendix A.2). This prompt ensures that the final answer is enclosed within a \\ boxed {} expression, facilitating automated extraction. We provide a detailed version of our scoring process in Appendix A.3.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/296", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 376.745, "r": 543.091, "b": 308.029, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 329]}]}], "headings": ["Baselines"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nWe first present our main results, comparing our approach against a set of strong baselines in Table 1. Inferencetime scaling results are based on a budget of 64 samples, with Qwen2.5-Math-PRM-7B serving as the reward model. Specifically, it is used as an ORM in WBoN and as a PRM otherwise.\n\u00b7 Among all inference-time scaling methods, PF consistently achieves the best performance , outperforming other scaling methods by a significant margin.\n\u00b7 PF with Llama-3.1-8B-Instruct outperforms its much larger counterpart, Llama-3.1-70B-Instruct , on\n1 We only consider DVTS but not beam search itself for two reasons. First, it has been reported by Beeching et al. (2024) to have a better performance than beam search when the budget is more than 16. Second, the implementation of beam search from the official release by Beeching et al. (2024) is slower than DVTS on the same budget.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/298", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 306.972, "t": 272.7950000000001, "r": 543.184, "b": 204.46699999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 291]}]}, {"self_ref": "#/texts/299", "parent": {"$ref": "#/groups/7"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 200.45600000000002, "r": 543.097, "b": 167.606, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 152]}]}, {"self_ref": "#/texts/300", "parent": {"$ref": "#/groups/7"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 164.591, "r": 541.44, "b": 143.67899999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 100]}]}, {"self_ref": "#/texts/301", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "footnote", "prov": [{"page_no": 6, "bbox": {"l": 307.44, "t": 134.452, "r": 541.443, "b": 75.07899999999995, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 334]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nClosed-Source LLMs, Method = . Closed-Source LLMs, MATH500 = . Closed-Source LLMs, AIME 2024 = . GPT-4o, Method = -. GPT-4o, MATH500 = 76.2. GPT-4o, AIME 2024 = 13.3. o1-preview, Method = -. o1-preview, MATH500 = 87.0. o1-preview, AIME 2024 = 40.0. Claude3.5-Sonnet, Method = -. Claude3.5-Sonnet, MATH500 = 78.3. Claude3.5-Sonnet, AIME 2024 = 16.0. Open-Source LLMs, Method = . Open-Source LLMs, MATH500 = . Open-Source LLMs, AIME 2024 = . Llama-3.1-70B-Instruct, Method = -. Llama-3.1-70B-Instruct, MATH500 = 65.7. Llama-3.1-70B-Instruct, AIME 2024 = 16.6.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "table", "prov": [{"page_no": 7, "bbox": {"l": 54.55161666870117, "t": 728.7860107421875, "r": 288.3479919433594, "b": 424.74346923828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nQwen2.5-Math-72B-Instruct, Method = -. Qwen2.5-Math-72B-Instruct, MATH500 = 82.0. Qwen2.5-Math-72B-Instruct, AIME 2024 = 30.0. Open-Source SLMs, Method = . Open-Source SLMs, MATH500 = . Open-Source SLMs, AIME 2024 = . Llama-3.2-1B-Instruct, Method = Pass@1. Llama-3.2-1B-Instruct, MATH500 = 26.8. Llama-3.2-1B-Instruct, AIME 2024 = 0.0. , Method = BoN. , MATH500 = 46.6. , AIME 2024 = 3.3. , Method = WBoN. , MATH500 = 47.8. , AIME 2024 = 3.3. , Method = DVTS. , MATH500 = 52.8. , AIME 2024 = 6.6. , Method = Ours - PF. , MATH500 = 59.6. , AIME 2024 = 10.0.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "table", "prov": [{"page_no": 7, "bbox": {"l": 54.55161666870117, "t": 728.7860107421875, "r": 288.3479919433594, "b": 424.74346923828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nLlama-3.1-8B-Instruct, Method = Pass@1. Llama-3.1-8B-Instruct, MATH500 = 49.9. Llama-3.1-8B-Instruct, AIME 2024 = 6.6. , Method = BoN. , MATH500 = 58.6. , AIME 2024 = 10.0. , Method = WBoN. , MATH500 = 59.0. , AIME 2024 = 10.0. , Method = DVTS. , MATH500 = 65.7. , AIME 2024 = 13.3. , Method = Ours - PF. , MATH500 = 74.4. , AIME 2024 = 16.6. Open-Source Math SLMs, Method = . Open-Source Math SLMs, MATH500 = . Open-Source Math SLMs, AIME 2024 = . Qwen2.5-Math-1.5B-Instruct, Method = Pass@1. Qwen2.5-Math-1.5B-Instruct, MATH500 = 70.0.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "table", "prov": [{"page_no": 7, "bbox": {"l": 54.55161666870117, "t": 728.7860107421875, "r": 288.3479919433594, "b": 424.74346923828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nQwen2.5-Math-1.5B-Instruct, AIME 2024 = 10.0. , Method = BoN. , MATH500 = 82.6. , AIME 2024 = 13.3. , Method = WBoN. , MATH500 = 82.8. , AIME 2024 = 13.3. , Method = DVTS. , MATH500 = 83.4. , AIME 2024 = 16.6. , Method = Ours - PF. , MATH500 = 85.4. , AIME 2024 = 23.3. Qwen2.5-Math-7B-Instruct, Method = Pass@1. Qwen2.5-Math-7B-Instruct, MATH500 = 79.6. Qwen2.5-Math-7B-Instruct, AIME 2024 = 16.6. , Method = BoN. , MATH500 = 83.0. , AIME 2024 = 20.0. , Method = WBoN. , MATH500 = 84.6. , AIME 2024 = 20.0. , Method = DVTS. , MATH500 = 85.4. ,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "table", "prov": [{"page_no": 7, "bbox": {"l": 54.55161666870117, "t": 728.7860107421875, "r": 288.3479919433594, "b": 424.74346923828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\nAIME 2024 = 20.0. , Method = Ours - PF. , MATH500 = 87.0. , AIME 2024 = 23.3\nTable 1. Results of various LLMs on MATH500 and AIME 2024 where bold indicates the best in each category and italic indicates the overall best. The table highlights the performance of Inference Scaling methods, where Qwen2.5-Math-PRM-7B was used as the Reward Model. Each inference scaling methods were run with a computational budget of 64 model generations. Notably, the Qwen2.5-Math-7B model, when scaled with inference-time compute, achieves performance on par with o1-preview in MATH500, further showcasing the power of inference-time scaling for competitive performance with smaller models.\nMATH500 and achieves parity on AIME 2024 , demonstrating the efficiency of our approach.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "table", "prov": [{"page_no": 7, "bbox": {"l": 54.55161666870117, "t": 728.7860107421875, "r": 288.3479919433594, "b": 424.74346923828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}]}, {"self_ref": "#/texts/303", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.893, "t": 413.703, "r": 290.933, "b": 307.376, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 596]}]}, {"self_ref": "#/texts/304", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 65.403, "t": 299.225, "r": 291.095, "b": 278.3299999999999, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 88]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.2. Main results\n- \u00b7 The best PF results with Qwen2.5-Math-1.5B-Instruct surpass GPT-4o on both datasets, while coming very close to the o1-preview model on the MATH500 benchmark while the Qwen2.5-Math-7B-Instruct is able to match the performance of o1-preview on MATH500, further underscoring the effectiveness of our method.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/305", "parent": {"$ref": "#/groups/8"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 7, "bbox": {"l": 55.44, "t": 275.3140000000001, "r": 291.095, "b": 206.59899999999993, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 307]}]}], "headings": ["5.2. Main results"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.3. Scaling with inference-time compute\nWe now zoom in on how PF scales with inference-time compute. Figure 2 shows the change of performance (in terms of accuracy) with an increasing computation budget ( N = 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 ) for all SLMs we consider. As we can see, PF scales 4-16x faster than the next best competitor DVTS, e.g. DVTS requires a budget of 32 to reach the same performance of PF with a budget of 8 with LLama-3.2-1B-Instruct and requires a budget of 128 to\nFigure 4. Results of ablation on 100 question subset comparing the performance of PF across various PRMs. We find that the Qwen PRM scales the most effectively across generations.\nFigure 5. Effect of different aggregation strategies for the process reward model Qwen2.5-Math-PRM-7B, evaluated on a 100question subset of the MATH500 dataset. The plot compares the commonly used aggregation strategies-Min, Last, and Product-against our proposed Model Aggregation method.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/307", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.972, "t": 170.36900000000003, "r": 291.183, "b": 78.13099999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 453]}]}, {"self_ref": "#/texts/327", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 7, "bbox": {"l": 306.938, "t": 604.365, "r": 541.438, "b": 574.75, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 179]}]}, {"self_ref": "#/texts/347", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 306.938, "t": 471.795, "r": 542.931, "b": 420.262, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 289]}]}], "headings": ["5.3. Scaling with inference-time compute"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.3. Scaling with inference-time compute\nreach the performance of PF with a budget of 8 with LLama3.1-8B-Instruct.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/348", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 307.44, "t": 411.723, "r": 543.098, "b": 391.216, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 73]}]}], "headings": ["5.3. Scaling with inference-time compute"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.4. Ablation study\nPerformance of different PRMs To investigate the impact of the choice of PRM on our method, in Figure 4 we present the results of an ablation study on a subset of 100 questions from the MATH500 dataset, where we compare the accuracy of our method across various reward functions as the number of particles increases. Qwen2.5Math-PRM-7B consistently outperforms other models, making it the natural choice for our main results. Interestingly, while EurusPRM-Stage2 performs relatively poorly with smaller budgets, it gradually improves and eventually matches Qwen2.5-Math-PRM-7B at higher budgets.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/350", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 307.082, "t": 356.37, "r": 543.098, "b": 227.87799999999993, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 595]}]}], "headings": ["5.4. Ablation study"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.4. Ablation study\nReward aggregation within PRMs As mentioned in Section 4.2 and reported by many previous works (Zhang et al., 2025b), there exist multiple ways to use PRMs to calculate reward scores which can have large impact on final performance. Figure 5 studies 3 existing ways to use a set of PRM scores-using the last reward, the minimum reward, and the product of all the rewards. We also study 'Model Aggregation', through which we use the PRM as an ORM with partial answers. As we can see, using Model Aggregation-in essence, feeding into a PRM the entire partial answer alongside the question - scales the best with an increasing budget.\nFigure 6. Results of using Llama 3.2 1B as our policy model across temperatures (0.4, 0.6, 0.8, 1.0, 1.2, and 1.4) and particle numbers (1, 2, 4, 8, 16, and 32).\nFigure 7. Comparison of PF and Particle Gibbs with different numbers of iterations, evaluated on a 100-question subset of the MATH500 dataset using Llama-3.2-1B-Instruct as the policy model.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/351", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 307.44, "t": 203.365, "r": 543.188, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 631]}]}, {"self_ref": "#/texts/353", "parent": {"$ref": "#/pictures/9"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 8, "bbox": {"l": 54.938, "t": 631.876, "r": 289.439, "b": 602.262, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 161]}]}, {"self_ref": "#/texts/375", "parent": {"$ref": "#/pictures/10"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 8, "bbox": {"l": 54.938, "t": 498.738, "r": 290.93, "b": 469.123, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 190]}]}], "headings": ["5.4. Ablation study"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.4. Ablation study\nControlling the state transition-temperatures in LLM generation We investigate the effect of different LM sampling temperatures on the scaling of our method across different numbers of particles. The results of our ablation study on a 100 question subset of MATH questions are shown in Figure 6. Our findings indicate that the commonly used range of llm temperature of 0.4-1.0 performs well, with minimal variations in accuracy across different budgets. Similar to Beeching et al. (2024), we set the temperature to 0.8 for all our experiments.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/394", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 55.082, "t": 460.972, "r": 291.179, "b": 344.435, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 543]}]}], "headings": ["5.4. Ablation study"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.5. Budget allocation over iterations and parallelism\nThe multi-iteration and parallel-chain extensions introduced in Section 4.2.1 provides two more axes to spend computation in addition to the number of particles. We now explore how different ways to allocate budgets changes the performance. Specifically, we study for a fixed budget N \u00d7 T \u00d7 M , how the combination of N,T,M can yield the best performance, where N is the number of particles, T is the number of iterations, and M is the number of parallelism.\nAllocating budget between N and T Figure 7 shows results of Llama-3.2 1B model when configured with various test-time compute budget allocations. Although the plot shows that various Particle Gibbs configurations do not have a marked benefit over an equivalently budgeted particle filtering run, a PG experiment with 16 particles and 4 iterations powered by a Qwen 2.5 7B Math Instruct policy model achieved a 87.2% accuracy on MATH500, beating o1 performance. Configurations with larger N values typically do better than equivalently budgeted runs with less particles.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/396", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 55.131, "t": 309.202, "r": 291.095, "b": 216.96399999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 458]}]}, {"self_ref": "#/texts/397", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 55.092, "t": 191.40999999999997, "r": 291.186, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 569]}]}], "headings": ["5.5. Budget allocation over iterations and parallelism"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "5.5. Budget allocation over iterations and parallelism\nFigure 8. Comparison of PF and PT with different particle group sizes, evaluated on a 100-question subset of the MATH500 dataset using Llama-3.2-1B-Instruct as the policy model.\nAllocating budget between N and M Figure 8 shows PF and 3 PT configurations over a set of increasing numbers of budgets. First, as we can see, for any fixed N , increasing M also improves the performance. This may be helpful when combining batch generation with distributed computing. Second, PT with N = 16 has a better overall scaling than PF. This indicates that there is some optimal budget allocation over parallel chains that can further improve the overall performance of our main results.\nWe leave the exploration over the optimal configuration of N,T,M jointly as a future work.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/398", "parent": {"$ref": "#/pictures/11"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 8, "bbox": {"l": 306.938, "t": 604.373, "r": 541.441, "b": 574.758, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 177]}]}, {"self_ref": "#/texts/418", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 307.082, "t": 566.607, "r": 543.096, "b": 462.026, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 496]}]}, {"self_ref": "#/texts/419", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 306.972, "t": 452.645, "r": 541.441, "b": 432.138, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 90]}]}], "headings": ["5.5. Budget allocation over iterations and parallelism"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "6. Conclusion\nIn this paper, we introduce a set of inference-time scaling algorithms with PRMs that leverage particle-based Monte Carlo methods. Our evaluation demonstrates that these algorithms consistently outperform search-based approaches by a significant margin.\nHowever, inference-time scaling comes with computational challenges. Hosting and running a reward model often introduces high latency, making the process more resourceintensive. Additionally, for smaller models, extensive prompt engineering is often required to ensure outputs adhere to the desired format. Finally, hyperparameters such as temperature are problem-dependent and may require extensive tuning across different domains.\nWe hope that the formal connection of inference scaling to probabilistic modeling that we established in this work will lead to systematic solutions for the current limitations of these methods and pave the way for bringing advanced probabilistic inference algorithms into LLM inference-time scaling in future work.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/421", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 307.44, "t": 392.752, "r": 543.093, "b": 336.379, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 253]}]}, {"self_ref": "#/texts/422", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 307.44, "t": 326.998, "r": 543.098, "b": 234.76, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 432]}]}, {"self_ref": "#/texts/423", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 306.972, "t": 225.38, "r": 541.691, "b": 157.05200000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 315]}]}], "headings": ["6. Conclusion"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nAI-MO. Aimo validation aime dataset. https: //huggingface.co/datasets/AI-MO/ aimo-validation-aime , 2023. Accessed:\n2025-01-24.\nAndrieu, C., Doucet, A., and Holenstein, R. Particle Markov Chain Monte Carlo Methods. Journal of the Royal Statistical Society Series B: Statistical Methodology , 72(3): 269-342, June 2010. ISSN 1369-7412, 1467-9868. doi: 10.1111/j.1467-9868.2009.00736.x.\nBeeching, E., Tunstall, L., and Rush, S. Scaling test-time compute with open models, 2024. URL https:// huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/426", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 703.53, "r": 292.429, "b": 671.068, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 115]}]}, {"self_ref": "#/texts/427", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 65.403, "t": 667.664, "r": 114.379, "b": 659.112, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}]}, {"self_ref": "#/texts/428", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 648.0, "r": 291.092, "b": 591.627, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 256]}]}, {"self_ref": "#/texts/429", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 580.515, "r": 290.635, "b": 536.097, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 177]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R\u00b4 e, C., and Mirhoseini, A. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling, July 2024.\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S., Yao, Y., Peng, H., Cheng, Y ., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards, 2025.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/430", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 524.984, "r": 290.823, "b": 480.567, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 173]}]}, {"self_ref": "#/texts/431", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 469.454, "r": 290.683, "b": 413.082, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 264]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nFeng, S., Kong, X., Ma, S., Zhang, A., Yin, D., Wang, C., Pang, R., and Yang, Y. Step-by-step reasoning for math problems via twisted sequential monte carlo, 2024. URL https://arxiv.org/abs/2410.01920 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/432", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 401.969, "r": 290.687, "b": 357.552, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 202]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/433", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 346.439, "r": 291.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1334]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nD., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzm\u00b4 an, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/433", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 346.439, "r": 291.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1334]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nJ., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V ., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/433", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 55.44, "t": 346.439, "r": 291.093, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1334]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nK., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., C \u00b8 elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nP., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nS., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V ., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y ., Babaei, Y ., Wen, Y ., Song, Y ., Zhang, Y., Li, Y., Mao, Y.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nCoudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nLoyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nHahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nI.-E., Gat, I., Weissman, J., Geboski, J., Kohli,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/434", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 316.825, "t": 721.463, "r": 543.098, "b": 79.28700000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3061]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nJ., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/436", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 64.935, "t": 721.463, "r": 291.098, "b": 210.79399999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2409]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nM., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/436", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 64.935, "t": 721.463, "r": 291.098, "b": 210.79399999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2409]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nKent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/436", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 64.935, "t": 721.463, "r": 291.098, "b": 210.79399999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2409]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nS., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/436", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 64.935, "t": 721.463, "r": 291.098, "b": 210.79399999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2409]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nIvanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y ., Zhang, Y ., Zhang, Y ., Adi, Y ., Nam, Y ., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/436", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 64.935, "t": 721.463, "r": 291.098, "b": 210.79399999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2409]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nGuan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking, January 2025.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361 .\nLew, A. K., Zhi-Xuan, T., Grand, G., and Mansinghka, V. K. Sequential monte carlo steering of large language models using probabilistic programs, 2023. URL https: //arxiv.org/abs/2306.03081 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/437", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 55.44, "t": 193.22900000000004, "r": 290.687, "b": 148.81100000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 180]}]}, {"self_ref": "#/texts/438", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 55.44, "t": 131.24599999999998, "r": 293.624, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 210]}]}, {"self_ref": "#/texts/439", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 721.463, "r": 544.429, "b": 677.045, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 191]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's Verify Step by Step, May 2023a.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's verify step by step, 2023b. URL https: //arxiv.org/abs/2305.20050 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/440", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 666.18, "r": 542.687, "b": 633.718, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 163]}]}, {"self_ref": "#/texts/441", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 622.852, "r": 544.429, "b": 578.435, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 199]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nLoula, J., LeBrun, B., Du, L., Lipkin, B., Pasti, C., Grand, G., Liu, T., Emara, Y., Freedman, M., Eisner, J., Cotterell, R., Mansinghka, V., Lew, A. K., Vieira, T., and O'Donnell, T. J. Syntactic and semantic control of large language models via sequential monte carlo. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/ forum?id=xoXn62FzD0 .\nMacKay, D. J. Information theory, inference and learning algorithms . Cambridge university press, 2003.\nMoral, P. D. Sequential Monte Carlo Methods for Dynamic Systems: Journal of the American Statistical Association: Vol 93, No 443. https://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10473765, 1997.\nMurphy, K. P. Machine learning: a probabilistic perspective . MIT press, 2012.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/442", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 567.57, "r": 543.098, "b": 475.332, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 399]}]}, {"self_ref": "#/texts/443", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 464.645, "r": 541.44, "b": 443.959, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 103]}]}, {"self_ref": "#/texts/444", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 433.094, "r": 604.24, "b": 376.721, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 203]}]}, {"self_ref": "#/texts/445", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 366.035, "r": 543.184, "b": 345.349, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 78]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nOpenAI, :, Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., Iftimie, A., Karpenko, A., Passos, A. T., Neitz, A., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar, A., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A., Mishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph, B., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B., McGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B., McKinzie, B., Eastman, B., Lugaresi,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/446", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 334.484, "r": 543.098, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1250]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nC., Bassin, C., Hudson, C., Li, C. M., de Bourcy, C., V oss, C., Shen, C., Zhang, C., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C., Roberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D., Farhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica, D., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung, E., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F., Such, F. P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F., von Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G., Chabot,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/446", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 334.484, "r": 543.098, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1250]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nG., Zhao, G., Brockman, G., Leclerc, G., Salman, H., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren, H., Lightman, H., Chung, H. W., Kivlichan, I., O'Connell, I., Osband, I., Gilaberte, I. C., Akkaya, I., Kostrikov, I.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/446", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 307.44, "t": 334.484, "r": 543.098, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1250]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nSutskever, I., Kofman, I., Pachocki, J., Lennon, J., Wei, J., Harb, J., Twore, J., Feng, J., Yu, J., Weng, J., Tang, J., Yu, J., Candela, J. Q., Palermo, J., Parish, J., Heidecke, J., Hallman, J., Rizzo, J., Gordon, J., Uesato, J., Ward, J., Huizinga, J., Wang, J., Chen, K., Xiao, K., Singhal, K., Nguyen, K., Cobbe, K., Shi, K., Wood, K., Rimbach, K., Gu-Lemberg, K., Liu, K., Lu, K., Stone, K., Yu, K., Ahmad, L., Yang, L., Liu, L., Maksin, L., Ho, L., Fedus, L., Weng, L., Li, L., McCallum,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/448", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 64.825, "t": 721.463, "r": 291.097, "b": 318.39, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1916]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nL., Held, L., Kuhn, L., Kondraciuk, L., Kaiser, L., Metz, L., Boyd, M., Trebacz, M., Joglekar, M., Chen, M., Tintor, M., Meyer, M., Jones, M., Kaufer, M., Schwarzer, M., Shah, M., Yatbaz, M., Guan, M. Y., Xu, M., Yan, M., Glaese, M., Chen, M., Lampe, M., Malek, M., Wang, M., Fradin, M., McClay, M., Pavlov, M., Wang, M., Wang, M., Murati, M., Bavarian, M., Rohaninejad, M., McAleese, N., Chowdhury, N., Chowdhury, N., Ryder, N., Tezak, N., Brown, N., Nachum, O., Boiko, O., Murk, O., Watkins, O.,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/448", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 64.825, "t": 721.463, "r": 291.097, "b": 318.39, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1916]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nChao, P., Ashbourne, P., Izmailov, P., Zhokhov, P., Dias, R., Arora, R., Lin, R., Lopes, R. G., Gaon, R., Miyara, R., Leike, R., Hwang, R., Garg, R., Brown, R., James, R., Shu, R., Cheu, R., Greene, R., Jain, S., Altman, S., Toizer, S., Toyer, S., Miserendino, S., Agarwal, S., Hernandez, S., Baker, S., McKinney, S., Yan, S., Zhao, S., Hu, S., Santurkar, S., Chaudhuri, S. R., Zhang, S., Fu, S., Papay, S., Lin, S., Balaji, S., Sanjeev, S., Sidor, S., Broda, T., Clark, A., Wang, T., Gordon,", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/448", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 64.825, "t": 721.463, "r": 291.097, "b": 318.39, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1916]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nT., Sanders, T., Patwardhan, T., Sottiaux, T., Degry, T., Dimson, T., Zheng, T., Garipov, T., Stasi, T., Bansal, T., Creech, T., Peterson, T., Eloundou, T., Qi, V ., Kosaraju, V., Monaco, V., Pong, V., Fomenko, V., Zheng, W., Zhou, W., McCabe, W., Zaremba, W., Dubois, Y., Lu, Y., Chen, Y., Cha, Y., Bai, Y ., He, Y ., Zhang, Y ., Wang, Y ., Shao, Z., and Li, Z. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/448", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 64.825, "t": 721.463, "r": 291.097, "b": 318.39, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1916]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nPareja, A., Nayak, N. S., Wang, H., Killamsetty, K., Sudalairaj, S., Zhao, W., Han, S., Bhandwaldar, A., Xu, G., Xu, K., Han, L., Inglis, L., and Srivastava, A. Unveiling the secret recipe: A guide for supervised fine-tuning small llms, 2024. URL https://arxiv.org/abs/ 2412.13337 .\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM TestTime Compute Optimally can be More Effective than Scaling Model Parameters, August 2024.\nSudalairaj, S., Bhandwaldar, A., Pareja, A., Xu, K., Cox, D. D., and Srivastava, A. Lab: Large-scale alignment for chatbots, 2024. URL https://arxiv.org/abs/ 2403.01081 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/449", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 55.44, "t": 304.917, "r": 291.094, "b": 236.58899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 282]}]}, {"self_ref": "#/texts/450", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 55.44, "t": 223.11599999999999, "r": 291.098, "b": 190.654, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 146]}]}, {"self_ref": "#/texts/451", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 55.44, "t": 177.18100000000004, "r": 290.687, "b": 132.764, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 170]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nSwendsen, R. H. and Wang, J.-S. Nonlinear filtering: Interacting particle resolution -ScienceDirect. https://www.sciencedirect.com/science/article/abs/pii/S0764444297847787, 1986.\nS\u00a8 arkk\u00a8 a, S. Bayesian Filtering and Smoothing . Institute of Mathematical Statistics Textbooks. Cambridge University Press, 2013.\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y ., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935 .\nXiong, W., Zhang, H., Jiang, N., and Zhang, T. An implementation of generative prm. https://github. com/RLHFlow/RLHF-Reward-Modeling , 2024.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/452", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 55.44, "t": 119.29099999999994, "r": 361.691, "b": 74.87300000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 179]}]}, {"self_ref": "#/texts/453", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 721.642, "r": 541.784, "b": 689.0, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 131]}]}, {"self_ref": "#/texts/454", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 677.627, "r": 543.181, "b": 633.21, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 212]}]}, {"self_ref": "#/texts/455", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 621.836, "r": 545.624, "b": 589.374, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 140]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 , 2024.\nYuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., Liu, Z., Zhou, B., Peng, H., Liu, Z., and Sun, M. Advancing llm reasoning generalists with preference trees, 2024. URL https: //arxiv.org/abs/2404.02078 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/456", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 578.001, "r": 543.184, "b": 521.628, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 274]}]}, {"self_ref": "#/texts/457", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 510.255, "r": 544.429, "b": 453.882, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 251]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nZhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301 , 2025a.\nZhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The Lessons of Developing Process Reward Models in Mathematical Reasoning, January 2025b.\nZhao, S., Brekelmans, R., Makhzani, A., and Grosse, R. Probabilistic inference in language models via twisted sequential monte carlo, 2024. URL https://arxiv. org/abs/2404.17546 .", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/458", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 442.509, "r": 542.687, "b": 398.091, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 204]}]}, {"self_ref": "#/texts/459", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 386.718, "r": 543.098, "b": 342.3, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 178]}]}, {"self_ref": "#/texts/460", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 330.927, "r": 545.624, "b": 286.51, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 179]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "References\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406 .\nFigure 9. Particle filtering for inference scaling in details. We initialize x particles with the 'first step' of an answer to a question. At every step, each particle p i is given a score s t i by the PRM, which is then used as a weight w t i to determine how likely that particle is to be resampled (evolved via a solid line) at the next step. A particle is deemed 'active' (green, in this diagram) until it generates an \u2329 EOS \u232a token, after which it is still able to be resampled (evolved via a dashed line) but is not evolved further. This process continues until all particles have completed their answers and become inactive (filled yellow).", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/461", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 307.44, "t": 275.13599999999997, "r": 543.188, "b": 230.71900000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 199]}]}, {"self_ref": "#/texts/498", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 54.938, "t": 595.951, "r": 541.44, "b": 544.132, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 647]}]}], "headings": ["References"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 2 Particle Gibbs for Inference-Time Scaling\nInput : same as Algorithm 1 with the number of Gibbs iterations T\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/500", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 65.403, "t": 491.502, "r": 330.569, "b": 482.546, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 65]}]}, {"self_ref": "#/texts/501", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 12, "bbox": {"l": 65.403, "t": 481.007, "r": 332.693, "b": 262.17100000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 777]}]}], "headings": ["Algorithm 2 Particle Gibbs for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 2 Particle Gibbs for Inference-Time Scaling\nRun Algorithm 1 to get a set of particles { x ( i ) 1: t } N i =1 for j = 1 , . . . , T do Compute rewards w = [\u02c6 r ( x (1) 1: t ) , . . . , \u02c6 r ( x ( N ) 1: t )] Compute softmax distribution \u03b8 = softmax( w ) Sample reference particle x ref 1: t := x ( j ) 1: t where j \u223c P ( j = i ) = \u03b8 Initialize N -1 particles { x ( i ) 1 \u223c p M ( \u00b7 | c ) } N -1 i =1 t \u2190 1 while not all particles stop do Update w = [\u02c6 r ( x (1) 1: t ) , . . . , \u02c6 r ( x ( N -1) 1: t ) , \u02c6 r ( x ref 1: t )] Compute softmax distribution \u03b8 = softmax( w ) Sample indices { j ( i ) t } N i =1 \u223c P t ( j = i ) = \u03b8 i Update the set of particles as { x ( j ( i ) t ) 1: t } N i =1 Transition { x (", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/501", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 12, "bbox": {"l": 65.403, "t": 481.007, "r": 332.693, "b": 262.17100000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 777]}]}], "headings": ["Algorithm 2 Particle Gibbs for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 2 Particle Gibbs for Inference-Time Scaling\ni ) t +1 \u223c p M ( \u00b7 | c, x ( i ) t +1 ) } N i =1 t \u2190 t +1 end while end for Return : the set of particles in the end\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/501", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 12, "bbox": {"l": 65.403, "t": 481.007, "r": 332.693, "b": 262.17100000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 777]}]}, {"self_ref": "#/texts/501", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 12, "bbox": {"l": 65.403, "t": 481.007, "r": 332.693, "b": 262.17100000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 777]}]}], "headings": ["Algorithm 2 Particle Gibbs for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "A.1. Algorithm details\nFor a set of parallel chains with temperatures T 1 > T 2 > . . . , at each iteration, we swap the states of every pair of neighboring chains k, k +1 with the following probability\n<!-- formula-not-decoded -->\nwhere \u03c0 k , \u03c0 k +1 are the two targets (with different temperatures) and x k , x k +1 are their states before swapping.\n```\ni\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/504", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 55.44, "t": 196.60300000000007, "r": 541.439, "b": 175.77800000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 179]}]}, {"self_ref": "#/texts/505", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "formula", "prov": [{"page_no": 12, "bbox": {"l": 218.243, "t": 141.78700000000003, "r": 542.107, "b": 116.83100000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 95]}]}, {"self_ref": "#/texts/506", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 55.082, "t": 83.74299999999994, "r": 496.553, "b": 74.11400000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 119]}]}, {"self_ref": "#/texts/507", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 12, "bbox": {"l": 332.694, "t": 423.196, "r": 335.513, "b": 417.102, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}]}], "headings": ["A.1. Algorithm details"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/510", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 13, "bbox": {"l": 65.403, "t": 706.819, "r": 477.616, "b": 385.553, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1094]}]}], "headings": ["Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling\nInput : same as Algorithm 2 with the number of parallel chains M and a list of temperature T 1 , . . . , T M for j = 1 , . . . , T do for k = 1 , . . . , M do if j = 1 then Run Algorithm 1 to get a set of particles { x ( i ) 1: t } N i =1 for chain k else Initialize N -1 particles { x ( i ) 1 \u223c p M ( \u00b7 | c ) } N -1 i =1 t \u2190 1 while not all particles stop do Update w = [\u02c6 r ( x (1) 1: t ) , . . . , \u02c6 r ( x ( N -1) 1: t ) , \u02c6 r ( x ref 1: t )] Compute softmax distribution \u03b8 = softmax( w /T k ) Sample indices { j ( i ) t } N i =1 \u223c P t ( j = i ) = \u03b8 i Update the set of particles as { x ( j ( i ) t ) 1: t } N i =1 Transition { x ( i ) t +1 \u223c p M ( \u00b7 | c, x ( i ) t", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/510", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 13, "bbox": {"l": 65.403, "t": 706.819, "r": 477.616, "b": 385.553, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1094]}]}], "headings": ["Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling\n+1 ) } N i =1 t \u2190 t +1 end while end if Compute rewards w = [\u02c6 r ( x (1) 1: t ) , . . . , \u02c6 r ( x ( N ) 1: t )] Compute softmax distribution \u03b8 = softmax( w /T k ) Sample reference particle x ref 1: t := x ( j ) 1: t where j \u223c P ( j = i ) = \u03b8 i end for for k = 1 , . . . , M -1 do Exchange the reference particle between chain k and k +1 with probability according to (4) end for end for Return : M set of particles in the end\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/510", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 13, "bbox": {"l": 65.403, "t": 706.819, "r": 477.616, "b": 385.553, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1094]}]}, {"self_ref": "#/texts/510", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 13, "bbox": {"l": 65.403, "t": 706.819, "r": 477.616, "b": 385.553, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1094]}]}], "headings": ["Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "A.2. Inference Prompt Template\n```\nEvaluation System Prompt Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] Regardless of the approach, always conclude with: Therefore, the final answer is: $\\boxed{answer}$. I hope it is correct. Where [answer] is just the final number or expression that solves the problem.\n```\n```\nPRM Input Format ## Step 1: [Concise description] [Brief explanation and calculations] <reward_token> ## Step 2: [Concise description] [Brief explanation and calculations] <reward_token> ## Step 3: [Concise description] [Brief explanation and calculations] <reward_token>\n```", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/512", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 13, "bbox": {"l": 71.031, "t": 336.926, "r": 537.281, "b": 104.37900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 590]}]}, {"self_ref": "#/texts/514", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "code", "prov": [{"page_no": 14, "bbox": {"l": 71.031, "t": 720.518, "r": 286.223, "b": 595.568, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 271]}]}], "headings": ["A.2. Inference Prompt Template"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "ORMInput Format\n## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] ## Step 3: [Concise description] [Brief explanation and calculations] <reward_token>", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/516", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 71.031, "t": 552.709, "r": 286.223, "b": 474.124, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 224]}]}], "headings": ["ORMInput Format"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
{"chunk": "A.3. Evaluation details\nParsing and scoring Following prior work on mathematical reasoning benchmarks (Yang et al., 2024), we apply their heuristic-based parsing and cleaning techniques to robustly extract the boxed expression. These heuristics account for variations in spacing, formatting inconsistencies, and other common artifacts observed in model outputs. For answer verification, we follow Beeching et al. (2024) and convert responses to canonical form. Ground truth and generated answers are transformed from LaTeX into SymPy expressions, simplified for normalization, and converted back to LaTeX. Exact match is determined using two criteria: numerical equality, where both expressions evaluate to the same floating-point value, and symbolic equality, where both are algebraically equivalent as SymPy expressions (Beeching et al., 2024). Accuracy is then computed as the fraction of problems where the generated answer exactly matches the ground truth.", "file": "inference-time-scaling", "metadata": {"schema_name": "docling_core.transforms.chunker.DocMeta", "version": "1.0.0", "doc_items": [{"self_ref": "#/texts/518", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 55.191, "t": 430.932, "r": 542.69, "b": 338.306, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 937]}]}], "headings": ["A.3. Evaluation details"], "origin": {"mimetype": "application/pdf", "binary_hash": 13049742986503704445, "filename": "inference-time-scaling.pdf"}}}
