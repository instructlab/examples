{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d378e39b-4de3-4578-867b-e142c5ce23d6",
   "metadata": {},
   "source": [
    "# Subset Selection on Text Chunks\n",
    "\n",
    "This notebook demonstrates how to perform subset selection on a set of text chunks specified in a `chunks.jsonl` file, with an example included in the `data/` subdirectory.\n",
    "\n",
    "**Note** that a GPU is required for this to run in a reasonable amount of time, and that **only Linux is supported** due to the usage of `faiss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b5b3c-cc68-4418-8b79-6caeddfeb1d1",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "First, we load the `chunks.jsonl` file. It is expected that the text chunks are in a key called `chunk`, with all other JSON key/values being metadata to have been preserved throughout this process.\n",
    "\n",
    "Example `chunk.jsonl` content:\n",
    "\n",
    "```\n",
    "{\"chunk\": \"this is the first chunk....\", \"created_date\": \"01-01-1970\", ... }\n",
    "{\"chunk\": \"this is the second chunk....\", \"created_date\": \"01-01-1971\", ... }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39792a86-af14-4ce8-bb25-287c0f16d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chunk_lookup = {}\n",
    "with open('data/chunks.jsonl', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        chunk_json = json.loads(line)\n",
    "        chunk_lookup[chunk_json['chunk']] = chunk_json\n",
    "\n",
    "print(f'Read {len(chunk_lookup)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dcbc4-097d-412f-a8b6-18ce4ee844fc",
   "metadata": {},
   "source": [
    "## Set up subset selection environment\n",
    "\n",
    "We begin by checking out the `[DataCurate4LLMs](https://github.com/krishnatejakk/DataCurate4LLMs) repository` and change into that directory.\n",
    "\n",
    "There has been a change to the dependencies of this project, so we will correct that locally and install dependencies.\n",
    "\n",
    "Finally, we create an `output/` folder to hold our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594d7bb-228b-468a-99a0-38c14651a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 git@github.com:krishnatejakk/DataCurate4LLMs.git\n",
    "%cd DataCurate4LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee552788-c2a7-4f7b-b1bb-728c98164e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i -e 's ^faiss-gpu$ faiss-gpu-cu12 g' requirements.txt # fix faiss dependency; yes you can use spaces as delimiters for sed expressions\n",
    "!pip install -qq -r requirements.txt\n",
    "!pip install -qq submodlib-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4239673-86ed-4ac4-9d21-5f32876a273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b628792-f62c-4740-a0e9-e55b6f369a28",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Next, we set up a configuration file for subset selection. We choose an embedding model, specify a simple template that directly uses chunks without modification for embedding, and use a random seed. We are creating it dynamically and then saving it to disk because of the random seed.\n",
    "\n",
    "The saved configuration file is then displayed below to inspection and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c8b60-daa9-48e4-b680-0b59588609ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = random.randint(0,10000)\n",
    "\n",
    "config = f\"\"\"{{\n",
    "    \"output_dir\": \"output\",\n",
    "    \"encoder_model\": \"BAAI/bge-large-en-v1.5\",\n",
    "    \"encoder_type\": \"bge\",\n",
    "    \"instruction\": \"Generate embeddings that capture the semantic meaning of text segments across multiple domains, ensuring g\\\n",
    "eneralization and suitability for clustering based on semantic similarity.\",\n",
    "    \"query_description\": \"default\",\n",
    "    \"templates\": {{\n",
    "        \"default\": \"{{{{ chunk }}}}\"\n",
    "    }},\n",
    "    \"template_name\": \"default\",\n",
    "    \"num_folds\": 1,\n",
    "    \"num_gpus\": 1,\n",
    "    \"subset_sizes\": [\"5\"],\n",
    "    \"epsilon\": 0.01,\n",
    "    \"seed\": {seed}\n",
    "}}\"\"\"\n",
    "\n",
    "config_path = 'subset_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0ba72-a1ca-4583-baf6-3da072b5ab96",
   "metadata": {},
   "source": [
    "## Perform subset selection\n",
    "\n",
    "Finally, we run the algorithm, save the selected 5 chunks to disk, and then read out that file. A list of all supported parameters is given in the [`DataCurate4LLMs` project `README`](https://github.com/krishnatejakk/DataCurate4LLMs/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b086790-5a9f-4e2f-b130-2d9f9f8a0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data_subset_selection.py --input_files '../data/chunks.jsonl' --output_dir 'output' --config 'subset_config.json' --retry_delay 1 --subset_sizes 5 --num_gpus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c625b-5ce1-4af2-ab5d-dd5e5ccbac3c",
   "metadata": {},
   "source": [
    "## Convert output back to `chunks.jsonl` format\n",
    "\n",
    "Finally, we read in the selected chunks, match the to the original data extracted from `chunks.jsonl` in order to preserve metadata, and save the final subset into a file in the same format as `chunks.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5622c-2101-47df-b366-2afb137cdd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('output/chunks/chunks_samples_5_subset.jsonl') as fin:\n",
    "    with open('output/selected_chunks.jsonl','w') as fout:\n",
    "        for line in fin.readlines():\n",
    "            selected_chunk = json.loads(line)['chunk']\n",
    "            original_chunk = chunk_lookup[selected_chunk]\n",
    "            fout.write(json.dumps(original_chunk) + \"\\n\")\n",
    "\n",
    "with open('../data/selected_chunks.jsonl') as final:\n",
    "    for line in final.readlines():\n",
    "        print(json.dumps(json.loads(line), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
