{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af99f876-0ffd-4079-aeb7-4cead05daaf4",
   "metadata": {},
   "source": [
    "# ðŸ¶ Data Pre-Processing\n",
    "\n",
    "This notebook goes through each of the stages of data pre-processing. Once a SDG seed dataset is created, a user can run through an SDG notebook and generate samples.\n",
    "\n",
    "1. [Document Conversion](#Document-Conversion)\n",
    "1. [Chunking](#Chunking)\n",
    "1. [Authoring](#Authoring)\n",
    "1. [Create Seed Dataset](#Create-Seed-Dataset-for-SDG)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_NAME = \"default\"\n",
    "\n",
    "WORKSPACE_ROOT = Path(\"workspaces\")\n",
    "WORKSPACE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "WORKSPACE_DIR = WORKSPACE_ROOT / WORKSPACE_NAME\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SOURCE_DOCUMENT = None # to process a specific document, set its path here; otherwise, the entire source documents repository will be used\n",
    "SOURCE_DOCUMENT_DIR = WORKSPACE_DIR / \"source_documents\"\n",
    "SOURCE_DOCUMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = WORKSPACE_DIR / \"conversion\"\n",
    "CONVERSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKING_OUTPUT_DIR = WORKSPACE_DIR / \"chunking\"\n",
    "CHUNKING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "AUTHORING_OUTPUT_DIR = WORKSPACE_DIR / \"authoring\"\n",
    "AUTHORING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_INPUT_DIR = WORKSPACE_DIR / \"sdg_inputs\"\n",
    "SEED_EXAMPLE_INPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_OUTPUT_DIR = WORKSPACE_DIR / \"seed_examples\"\n",
    "SEED_EXAMPLE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SDG_OUTPUT_DIR = WORKSPACE_DIR / \"sdg\"\n",
    "SDG_OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b7ac5-fc2a-40a8-8e1f-e8dd8b1153e7",
   "metadata": {},
   "source": [
    "## Document Conversion\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d4b2e-19cd-46e7-a912-ba9b2904c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3804ef-4961-44b1-91c9-62929f422702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** WARNING! Only one file at a time is supported at this time.\n",
      "***** Using workspaces/default/source_documents/2502.01618v3.pdf)\n",
      "Files to convert: [PosixPath('workspaces/default/source_documents/2502.01618v3.pdf')]\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "if SOURCE_DOCUMENT:\n",
    "    files.append(Path(SOURCE_DOCUMENT))\n",
    "else:\n",
    "    print(\"***** WARNING! Only one file at a time is supported at this time.\")\n",
    "    files = list(SOURCE_DOCUMENT_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"***** Using {files[0]})\")\n",
    "\n",
    "print(f\"Files to convert: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fb64b-d089-4844-9330-7f3639819e7a",
   "metadata": {},
   "source": [
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157c5e02-edd1-44f6-b20f-f6b4bda1aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pipeline_options = PdfPipelineOptions() # TODO: show the options that can be set\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400c74-dead-4998-aee2-ddb00ddaa276",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a200039c-b8b2-4087-88ba-7bfb0e393cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of JSON output is: /Users/astoyano/Documents/code/examples/notebooks/instructlab-knowledge/workspaces/default/conversion/2502.01618v3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in files:\n",
    "    doc = doc_converter.convert(source=file).document\n",
    "    doc_dict = doc.export_to_dict()\n",
    "\n",
    "    json_output_path = CONVERSION_OUTPUT_DIR / f\"{file.stem}.json\"\n",
    "    with open(json_output_path, \"w\") as f:\n",
    "        json.dump(doc_dict, f)\n",
    "        print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "The goal of chunking the converted documents is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "chunker = HybridChunker() # TODO: expose configuration options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce1d6f-b8d3-470c-b3c9-675911f0ee92",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 52 chunks from 2502.01618v3\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "docs = []\n",
    "for file in files:\n",
    "    doc = DocumentConverter().convert(source=file)\n",
    "    docs.append(doc)\n",
    "    \n",
    "    chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "    chunk_objs = list(chunk_iter)\n",
    "    chunks = [chunker.contextualize(chunk=chunk) for chunk in chunk_objs]\n",
    "\n",
    "    print(f\"Extracted {len(chunks)} chunks from {doc.document.name}\")\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        c = dict(chunk=chunk, file=file.stem)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "# TODO: support multiple files save all chunks to single file for review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "### View the Chunks\n",
    "\n",
    "To view the chunks, run through the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\n",
      "Isha Puri 1 Shivchander Sudalairaj 2 Guangxuan Xu 2 Kai Xu 2 Akash Srivastava 2 1 MIT CSAIL 2 Red Hat AI Innovation\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "#print(all_chunks)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4160f-7508-4c72-b28d-b56aa4975b26",
   "metadata": {},
   "source": [
    "### Save the chunks to a text file for each chunk\n",
    "\n",
    "Each chunk is saved to an individual text file in the format: `{docling-json-file-name}-{chunk #}.txt`. Having chunking in this format is important as an input to create-sdg-seed-data notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e70d576-a2bc-4274-b660-1cbe051968b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk_path = CHUNKING_OUTPUT_DIR / f\"{chunk['file']}-{i}.txt\"\n",
    "    with open(chunk_path, \"w\") as file:\n",
    "        file.write(chunk[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f8c7-8cd3-4867-8742-9f4f9cda9e9f",
   "metadata": {},
   "source": [
    "## Authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86c48e52-cda7-48ac-84dc-0b844aed5f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq docling-sdg\n",
    "\n",
    "# TODO: replace with above after https://github.com/docling-project/docling-sdg/pull/31 merges\n",
    "#!pip install -qq git+https://github.com/anastasds/docling-sdg@d15de2c5a81bfe166f66f412fc4b23728065f396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and filtering document 2502.01618v3\n",
      "Created dataset 2502.01618v3 with 45 QA chunks\n"
     ]
    }
   ],
   "source": [
    "from docling_sdg.qa.utils import get_qa_chunks\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunk_objs, filters)) #TODO: decouple reference to chunk_objs from above)\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "### Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProvider\n",
    "from pydantic import SecretStr\n",
    "\n",
    "generate_options = GenerateOptions(api_key=\"fake\", project_id=\"project_id\")\n",
    "generate_options.provider = LlmProvider.OPENAI_LIKE\n",
    "generate_options.api_key = SecretStr(\"fake\")\n",
    "generate_options.model_id = \"granite3.3\"\n",
    "\n",
    "generate_options.api_key = SecretStr(\"a8230601c7cfc3c891ab744108417f8e\")\n",
    "generate_options.url = \"https://mixtral-8x7b-instruct-v0-1-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\"\n",
    "generate_options.model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919199c0-3747-409a-85ab-0155ef3ebe9d",
   "metadata": {},
   "source": [
    "### Configure subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1197d4e-8354-45e3-9ec9-85c78ba36548",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_SELECT_FOR_AUTHORING = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421d07-3e6c-4355-95f4-da8e157557c7",
   "metadata": {},
   "source": [
    "### Run QA generation on selected chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks that looks like:\n",
      "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT4o accuracy in only 4 rollouts, while Qwen2.5Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at probabilistic-inference-scaling.github.io/ .\n",
      "Selected 5 contexts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502.01618v3: Status.SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random #TODO: replace random sampling with subset selection\n",
    "\n",
    "for doc, chunks in dataset.items(): # TODO: multiple file support\n",
    "    generate_options.generated_file = AUTHORING_OUTPUT_DIR / f\"qagen-{doc}.json\" \n",
    "    gen = Generator(generate_options=generate_options)\n",
    "    \n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    selected_chunks = random.sample(chunks, NUM_CHUNKS_TO_SELECT_FOR_AUTHORING)\n",
    "    print(f\"Selected {len(selected_chunks)} contexts\")\n",
    "\n",
    "    Path.unlink(generate_options.generated_file, missing_ok=True)\n",
    "    results = gen.generate_from_chunks(selected_chunks) # automatically saves to file\n",
    "    \n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "### Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA pairs for 5 contexts\n",
      "[{'question': 'What is the name of the paper that presents a novel inference-time scaling approach for large language models?', 'answer': 'The name of the paper is not mentioned in the provided context.'}, {'question': 'What are the main advantages of the proposed inference-time scaling approach compared to existing methods?', 'answer': 'The proposed approach has a 4-16x better scaling rate over deterministic search counterparts on various mathematical reasoning tasks, and it can surpass GPT4o accuracy in only 4 rollouts for Qwen2.5-Math-1.5B-Instruct and scale to o1 level accuracy in only 32 rollouts for Qwen2.5Math-7B-Instruct.'}, {'question': 'Given the empirical evaluation results, how would you infer the computational cost of the proposed approach compared to the existing inference-time scaling methods?', 'answer': 'Based on the provided context, the computational cost of the proposed approach is significantly less than that of the existing methods, as it demonstrates a 4-16x better scaling rate and requires fewer rollouts to achieve the same or better accuracy levels.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(f\"Generated QA pairs for {len(qnas)} contexts\")\n",
    "print(list(qnas.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6c26-f4d5-420d-ae78-ac28cf39efd3",
   "metadata": {},
   "source": [
    "### Define metadata for qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7130e90-2b65-4008-86f7-194da74a9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_OUTLINE = \"A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\"\n",
    "DOMAIN = \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "### Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "qna_output_path = AUTHORING_OUTPUT_DIR / \"qna.yaml\"\n",
    "\n",
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "data['document_outline'] = DOCUMENT_OUTLINE\n",
    "data['domain'] = DOMAIN\n",
    "\n",
    "Path.unlink(qna_output_path, missing_ok=True) # shouldn't be necessary but was. jupyter caching thing?\n",
    "with open(qna_output_path, 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ea149-844b-4330-90ec-d0ca7ab12b90",
   "metadata": {},
   "source": [
    "### View generated qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1293d445-b826-4b92-ad20-9b121ac60e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_examples:\n",
      "  - context: |-\n",
      "      Large language models (LLMs) have achieved significant performance gains via\n",
      "      scaling up model sizes and/or data. However, recent evidence suggests\n",
      "      diminishing returns from such approaches, motivating scaling the computation\n",
      "      spent at inference time. Existing inference-time scaling methods, usually with\n",
      "      reward models, cast the task as a search problem, which tends to be vulnerable\n",
      "      to reward hacking as a consequence of approximation errors in reward models. In\n",
      "      this paper, we instead cast inference-time scaling as a probabilistic inference\n",
      "      task and leverage sampling-based techniques to explore the typical set of the\n",
      "      state distribution of a state-space model with an approximate likelihood, rather\n",
      "      than optimize for its mode directly. We propose a novel inference-time scaling\n",
      "      approach by adapting particle-based Monte Carlo methods to this task. Our\n",
      "      empirical evaluation demonstrates that our methods have a 4-16x better scaling\n",
      "      rate over our deterministic search counterparts on various challenging\n",
      "      mathematical reasoning tasks. Using our approach, we show that\n",
      "      Qwen2.5-Math-1.5B-Instruct can surpass GPT4o accuracy in only 4 rollouts, while\n",
      "      Qwen2.5Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our\n",
      "      work not only presents an effective method to inference-time scaling, but also\n",
      "      connects the rich literature in probabilistic inference with inference-time\n",
      "      scaling of LLMs to develop more robust algorithms in future work. Code, videos,\n",
      "      and further information available at probabilistic-inference-scaling.github.io/\n",
      "      .\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the name of the paper that presents a novel inference-time scaling\n",
      "          approach for large language models?\n",
      "        answer: The name of the paper is not mentioned in the provided context.\n",
      "      - question: |-\n",
      "          What are the main advantages of the proposed inference-time scaling approach\n",
      "          compared to existing methods?\n",
      "        answer: |-\n",
      "          The proposed approach has a 4-16x better scaling rate over deterministic search\n",
      "          counterparts on various mathematical reasoning tasks, and it can surpass GPT4o\n",
      "          accuracy in only 4 rollouts for Qwen2.5-Math-1.5B-Instruct and scale to o1 level\n",
      "          accuracy in only 32 rollouts for Qwen2.5Math-7B-Instruct.\n",
      "      - question: |-\n",
      "          Given the empirical evaluation results, how would you infer the computational\n",
      "          cost of the proposed approach compared to the existing inference-time scaling\n",
      "          methods?\n",
      "        answer: |-\n",
      "          Based on the provided context, the computational cost of the proposed approach\n",
      "          is significantly less than that of the existing methods, as it demonstrates a\n",
      "          4-16x better scaling rate and requires fewer rollouts to achieve the same or\n",
      "          better accuracy levels.\n",
      "  - context: |-\n",
      "      How to aggregate the step-level rewards remains a choice when one uses PRMs. There are three common ways to assign rewards to a partial answer using PRMs: prod , which takes the product of rewards across all steps; min , which selects the minimum reward over all steps; and last , which uses the reward from the final step. Zhang et al. (2025b) studies the optimal way for reward aggregation and points out that the 'best choice' depends on if the PRM training data is prepared using MC rollout and/or human/model annotation. While prod aligns directly with the weight update rule described earlier, min and last do not allow for online weight updates. Therefore, for these methods, we compute the weight based on the entire partial trajectory instead.\n",
      "      Beyond these three approaches, we also explored a modelbased reward aggregation method that performed surprisingly well. This method feeds the PRM with partial answers but only considers the final reward token, effectively prompting the model to provide an aggregated reward for the partial answer. Interestingly, we tested the Qwen PRM both for its original purpose as a true process reward model and repurposed as an outcome reward model. When used as a true PRM, it receives the question and a list of steps generated by the policy model, calculates scores for each step and selects the last score-a practice introduced and evaluated in Beeching et al. (2024). As an ORM, the PRM takes in a question and a concatenated string of generated steps, producing a score that we convert into a weight for the resampling process. Appendix A.2 provides an illustration of how the two input formats are structured. We compare various reward models and evaluate all four aggregation strategies through an ablation study in Section 5.4.\n",
      "      With the above defined, particle filtering iterates over the two steps below with a set of N particles at each iteration t\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the name of the model-based reward aggregation method that performed\n",
      "          surprisingly well?\n",
      "        answer: |-\n",
      "          The model-based reward aggregation method that performed surprisingly well is\n",
      "          not named in the passage.\n",
      "      - question: |-\n",
      "          What are the three common ways to assign rewards to a partial answer using PRMs\n",
      "          and how do they differ in their alignment with the weight update rule?\n",
      "        answer: |-\n",
      "          The three common ways to assign rewards to a partial answer using PRMs are prod,\n",
      "          min, and last. Prod aligns directly with the weight update rule described\n",
      "          earlier, while min and last do not allow for online weight updates and require\n",
      "          computing the weight based on the entire partial trajectory instead.\n",
      "      - question: |-\n",
      "          Given the description of the PRM used as a true process reward model and\n",
      "          repurposed as an outcome reward model, how does the input format and the PRM's\n",
      "          function differ in these two uses?\n",
      "        answer: |-\n",
      "          As a true PRM, the input is the question and a list of steps generated by the\n",
      "          policy model, and the PRM calculates scores for each step and selects the last\n",
      "          score. As an ORM, the input is a question and a concatenated string of generated\n",
      "          steps, and the PRM produces a score that is converted into a weight for the\n",
      "          resampling process.\n",
      "  - context: |-\n",
      "      Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's verify step by step, 2023b. URL https: //arxiv.org/abs/2305.20050 .\n",
      "      Loula, J., LeBrun, B., Du, L., Lipkin, B., Pasti, C., Grand, G., Liu, T., Emara, Y., Freedman, M., Eisner, J., Cotterell, R., Mansinghka, V., Lew, A. K., Vieira, T., and O'Donnell, T. J. Syntactic and semantic control of large language models via sequential monte carlo. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/ forum?id=xoXn62FzD0 .\n",
      "      MacKay, D. J. Information theory, inference and learning algorithms . Cambridge university press, 2003.\n",
      "      Moral, P. D. Sequential Monte Carlo Methods for Dynamic Systems: Journal of the American Statistical Association: Vol 93, No 443. https://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10473765, 1997.\n",
      "      Murphy, K. P. Machine learning: a probabilistic perspective . MIT press, 2012.\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          Who is one of the authors of the paper 'Let's verify step by step' published in\n",
      "          2023?\n",
      "        answer: Lightman, H.\n",
      "      - question: What are the common themes in the provided list of references?\n",
      "        answer: |-\n",
      "          The common themes in the provided list of references are information theory,\n",
      "          learning algorithms, and sequential Monte Carlo methods.\n",
      "      - question: |-\n",
      "          Based on the titles and authors of the papers, can you infer a potential\n",
      "          collaboration or competition between the authors of the two papers 'Let's verify\n",
      "          step by step' and 'Syntactic and semantic control of large language models via\n",
      "          sequential monte carlo'?\n",
      "        answer: |-\n",
      "          Without additional information, it's not possible to definitively infer a\n",
      "          relationship between the authors or their papers. They could be collaborating,\n",
      "          but it's also possible that they are working independently on related topics.\n",
      "  - context: |-\n",
      "      Parsing and scoring Following prior work on mathematical reasoning benchmarks\n",
      "      (Yang et al., 2024), we apply their heuristic-based parsing and cleaning\n",
      "      techniques to robustly extract the boxed expression. These heuristics account\n",
      "      for variations in spacing, formatting inconsistencies, and other common\n",
      "      artifacts observed in model outputs. For answer verification, we follow Beeching\n",
      "      et al. (2024) and convert responses to canonical form. Ground truth and\n",
      "      generated answers are transformed from LaTeX into SymPy expressions, simplified\n",
      "      for normalization, and converted back to LaTeX. Exact match is determined using\n",
      "      two criteria: numerical equality, where both expressions evaluate to the same\n",
      "      floating-point value, and symbolic equality, where both are algebraically\n",
      "      equivalent as SymPy expressions (Beeching et al., 2024). Accuracy is then\n",
      "      computed as the fraction of problems where the generated answer exactly matches\n",
      "      the ground truth.\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the method used for transforming ground truth and generated answers into\n",
      "          SymPy expressions?\n",
      "        answer: |-\n",
      "          The method for transforming ground truth and generated answers into SymPy\n",
      "          expressions is not specified in the passage.\n",
      "      - question: |-\n",
      "          What are the two criteria used for determining exact match between generated and\n",
      "          ground truth answers?\n",
      "        answer: |-\n",
      "          The two criteria used for determining exact match between generated and ground\n",
      "          truth answers are numerical equality and symbolic equality.\n",
      "      - question: |-\n",
      "          Based on the information provided, can we infer that the heuristics used for\n",
      "          parsing and cleaning techniques are specific to the mathematical reasoning\n",
      "          benchmarks mentioned?\n",
      "        answer: |-\n",
      "          Yes, according to the passage, the heuristics are specific to the mathematical\n",
      "          reasoning benchmarks mentioned.\n",
      "  - context: |-\n",
      "      Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 , 2024.\n",
      "      Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., Liu, Z., Zhou, B., Peng, H., Liu, Z., and Sun, M. Advancing llm reasoning generalists with preference trees, 2024. URL https: //arxiv.org/abs/2404.02078 .\n",
      "      Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301 , 2025a.\n",
      "      Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The Lessons of Developing Process Reward Models in Mathematical Reasoning, January 2025b.\n",
      "      Zhao, S., Brekelmans, R., Makhzani, A., and Grosse, R. Probabilistic inference in language models via twisted sequential monte carlo, 2024. URL https://arxiv. org/abs/2404.17546 .\n",
      "      Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406 .\n",
      "    questions_and_answers:\n",
      "      - question: Which paper was published in January 2025 by Zhang, Z. and others?\n",
      "        answer: The Lessons of Developing Process Reward Models in Mathematical Reasoning\n",
      "      - question: |-\n",
      "          Which authors have published multiple papers in the field of mathematical\n",
      "          reasoning and language models in 2024 and 2025?\n",
      "        answer: |-\n",
      "          Zhang, Z., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. have each published\n",
      "          multiple papers in the field of mathematical reasoning and language models in\n",
      "          2024 and 2025.\n",
      "      - question: |-\n",
      "          Based on the provided context, which language model paper introduces a new\n",
      "          method for reasoning that combines acting and planning?\n",
      "        answer: |-\n",
      "          Language agent tree search unifies reasoning acting and planning in language\n",
      "          models, as presented in the paper by Zhou, A., Yan, K., Shlapentokh-Rothman, M.,\n",
      "          Wang, H., and Wang, Y.-X.\n",
      "document_outline: |-\n",
      "  A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using\n",
      "  Particle-Based Monte Carlo Methods\n",
      "domain: artificial intelligence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(qna_output_path) as yaml_file:\n",
    "    print(yaml_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f101076-a50f-49ea-a83b-46eaa8b39cc4",
   "metadata": {},
   "source": [
    "## Create Seed Dataset for SDG\n",
    "\n",
    "This notebook combines the contents from the qna.yaml and the chunks from the source document to create a seed dataset for the synthetic data generation process.\n",
    "\n",
    "To run this notebook you need a directory that contains N chunks named `{original-file-name}-{N}.txt` and a `qna.yaml` in the same directory.\n",
    "\n",
    "This notebook outputs a `seed.jsonl` file in the `output_dir` that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2c6e31b-e8a9-406c-b2dc-27433c8fd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab2c9ed2-8ba8-4959-8e01-81625b81d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5dacd650f24959ae70be7303607d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea273d0d80ba448a91c6e75f7a466609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f168b10ddede49b792e2ad35a60d8a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeba8acf9094d51a8afb56b45237459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de0318675a4beeb42cf79ad0442e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c6ad63a2274252863097ae9bf772fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cc43930e2c40378ef277c1e9a75d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6aee9fda94acfb86baa0a3c8aa024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "924266"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.create_seed_dataset import get_seed_dataset\n",
    "\n",
    "src_files = os.listdir(CHUNKING_OUTPUT_DIR)\n",
    "\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(CHUNKING_OUTPUT_DIR, file_name)\n",
    "    if os.path.isfile(full_file_name):\n",
    "        shutil.copy(full_file_name, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "shutil.copy(qna_output_path, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "seed_data = get_seed_dataset(SEED_EXAMPLE_INPUT_DIR)\n",
    "seed_data.to_json(f'{SEED_EXAMPLE_OUTPUT_DIR}/seed_data.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
