{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da70a08-1895-4d1f-8f50-93e2134b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design goals:\n",
    "#\n",
    "# - understandability\n",
    "# - modularity\n",
    "# - configurability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "WORKSPACE_DIR=\"workspaces/default\"\n",
    "# replace with pathlib\n",
    "\n",
    "# mkdir etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dbcdf-93d7-474f-9826-77866ceb815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = f\"{WORKSPACE_DIR}/conversion\"\n",
    "# replace with pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bbcbd-9bdd-474e-9d44-5c9d5a2c03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authoring\n",
    "\n",
    "# TODO: plug into docs\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunks, filters))\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "#Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b50a7-889f-4432-a29b-5a8fd55177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling-sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProviders\n",
    "\n",
    "generate_options = GenerateOptions(api_key=\"fake\", project_id=\"project_id\")\n",
    "generate_options.provider = LlmProviders.OPENAI_LIKE\n",
    "generate_options.api_key = \"fake\"\n",
    "generate_options.model_id = \"mixtral\" # for local ollama\n",
    "generate_options.generated_file = f\"data/chunks-{filename_base}.jsonl\"\n",
    "\n",
    "gen = Generator(generate_options=generate_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, chunks in dataset.items():\n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    results = gen.generate_from_chunks(chunks)\n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "# Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(qnas[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "# Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "with open('qna.yml', 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149dbbf-5601-4aa5-b1e9-e454ea0a4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
